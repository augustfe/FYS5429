The resolution of Partial Differential Equations (PDEs) underpins many scientific and engineering challenges.
PDEs model the behaviour of a system of multiple variables with respect to its derivatives, where the problem at hand is to determine the function, rather than a number.
It is often cumbersome, or impossible, to determine the analytical solution of a PDE arising from real-world conditions, which has led to the development of numerous numerical methods.
A benefit of these numerical methods, is that the theoretical groundwork behind them has been extensively explored, making them more analytically transparent.
However, the computational and analytical burdens associated with implementing such methods are often costly.
As neural networks have a proven capacity of being able to act as universal approximators of functions \cite{Cybenko1989ApproximationBS}, it is therefore natural to explore whether they are capable of solving PDEs.

A large part of the recent advancements in machine learning can be contributed to a higher degree of data availability and increased computational resources.
With problems involving PDEs however, data acquisition can be an issue in and of itself.
Due to this, recent methods combat this through incorporating the underlying knowledge we have about the system directly into the training of the neural network, in a framework aptly named Physics-Informed Neural Networks (PINNs) in \textcite{RAISSI2019686}.
With these networks, we gain a greater degree of certainty regarding the accuracy of the predictions, as they inherently attempt to abide by the physical laws of the system.

This is primarily done through punishing the PINN in the training stage for not adhering to the PDE conditions.
However, there are other facets of a problem one might concentrate on in order to increase predictive power.
One of these stem from the inherent discontinuities present in certain problems, or the wildly differing behaviours across the spatial- or temporal dimension(s).
A number of extensions of PINNs have therefore been proposed, including but not limited to, Conservative PINNs (cPINNs) \cite{2020CMAME.36513028J}, Extended PINNs (XPINNs) \cite{Jagtap2020ExtendedPN} and Augmented PINNs (APINNs) \cite{Hu_2023}.
These methods primarily focus on various forms of domain decomposition.

We focus our attention in this report on Extended Physics-Informed Neural Networks.
In this method, the domain is decomposed into subdomains consisting of hard boundaries, with a PINN trained on each subdomain.
The goal of this method is to capture more homogeneous behaviour, in order to reduce the necessary complexity of the network and increase convergence speeds.
We attempt to reproduce some of the results from \textcite{XPINN_generalize}.

In order to achieve this, we provide a framework for the implementation of such problems, with a focus on modularity in order to accommodate for a wide variety of problems.
We therefore utilize \verb|JAX| \cite{jax2018github} in order to achieve good performance in \verb|Python|, with \verb|Optax| \cite{deepmind2020jax} in order to perform the optimization steps.

We begin by covering the necessary preliminaries of neural networks and PDEs, before covering how these topics combine in PINNs.
Next, we introduce XPINNs, before explaining how automatic differentiation functions.
Following, we cover the necessary methodology used within our implementation, such as utilizing \textsc{Adam} as our optimizer.
Finally, we test our implementation on the advection, Poisson and Navier-Stokes equations, comparing the performance against a typical PINN.
