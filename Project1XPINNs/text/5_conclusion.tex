Extended Physics-Informed Neural Networks represent an interesting development within machine learning.
Computational models have a tendency to adapt themselves to the constraint placed upon them by the computer architecture available at the time, from early CPU limitations to later memory constraints.
Currently, the best performing models are trained in large data centers, on huge datasets.
Improvements largely stem from the ability to train more complex models longer on more data, however we are seeing the early stages of being limited by environmental factors.

Large Language Models (LLMs), such as LLaMa \cite{touvron2023llama}, contain billions of parameters.
Recent work has explored 1.58bit models \cite{ma2024era}, where each weight $W_{ij}$ is constrained to be in $\{-1, 0, 1\}$, in order to reduce the size of the models.
This has the potential benefit of reducing the energy usage associated with training the models.
This exemplifies the direction machine learning seems to be moving, wherein models are specialized based on the problem at hand, rather than being basic approximators.

XPINNs seem to, based on our results, not be the final stage in the evolution of PINNs.
The limitations stemming from the reduction in available data, as well as the introduction of interface losses, seem to not be outweighed by the solutions we were able to produce.
Augmented Physics-Informed Neural Networks (APINNs), introduced in \textcite{Hu_2023}, act as a natural evolution of XPINNs.
With APINNs, gating functions serve as the equivalent of the domain decomposition presented in this paper.
Using this methodology, each sub-PINN has access to all of the training data, and there are no hard boundaries introduced.

This is not to say that XPINNs are without merit, however we believe that further work should rather be placed on APINNs.
Another avenue for exploration is within the chosen optimizer.
Methods such as Energy Natural Gradient Descent \cite{m√ºller2023achieving} have performed well in conjunction with PINNs, while learning rate free algorithms based on coin-betting ideas \cite{sharrock2023learning} perform remarkably well without the necessity of tuning the learning rate.
