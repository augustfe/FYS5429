% Discussion
From the results on the advection equation we demonstrated that XPINNs can capture the wave's propagation, including handling the predicted discontinuities, which suggest they can reliably find solutions close to the analytical benchmarks.
They are still slightly outperformed by a less computationally costly single PINN, however our aim of this demonstration was to show that they give comparable results.
XPINNs are evidently superfluous for simple problems, but have adequate performance to be considered for more complex problems.

In the case of the Poisson equation, our findings indicate that while XPINNs can manage both continuous and discontinuous residuals.
As expected the XPINN underperforms for solving the Poisson equation.
We also get quantitatively worse results than \textcite{XPINN_generalize}, however this can likely be contributed to the fact that we use \textsc{Adam} as our optimizer while they utilize LBFGS, which is a quasi-Newton method expected to perform better.
Our results are qualitatively not too far off from the analytical solution, however they are again outperformed by a singular PINN.

One of the primary benefits of XPINNs is the inherent capacity for parallelization.
However, we were not able to implement this, meaning we were not able to get a computational performance boost while training the XPINNs, other than the reduced complexity utilized with Navier-Stokes.
In addition, introducing interface losses might complicate the loss landscape entirely, such that more powerful optimization algorithms are necessary.
These methods include Energy Natural Gradient Descent \cite{m√ºller2023achieving}, which yielded orders of magnitude lower errors, and learning rate free algorithms \cite{sharrock2023learning}, which alleviate the need to tune the learning rate as a parameter while performing remarkably well.

This leads us to another challenge we ran into with the XPINN, wherein we introduce a number of new hyperparameters through the MSE in \eqref{eq:PINNMSE}, which needs to be balanced against the interface losses.
This issue is unavoidable with XPINNs, leading to a more complex tuning stage.
\textcite{Hu_2023} attempts to alleviate this through the introduction of Augmented PINNs (APINNs), which present a natural evolution of XPINNs.
Here, the domain is decomposed through the use of gating functions, rather than hard boundaries.
Thus, interface losses are eliminated entirely, in addition to each sub-PINN gaining access to all of the available training data.
We believe further work should be focused on exploring and utilizing APINNs, rather than optimizing XPINNs further.
% Possible amelioratons

Another avenue for exploration is within the realm of how the weights of the network are set.
Recently, 1.58bit models \cite{ma2024era} have been developed, where in each weight $W_{ij}$ of the networks in constrained to be in $\{-1, 0, 1\}$, rather than the 32-bit or 64-bit floating point numbers used today.
This greatly reduce the physical size of networks, while seemingly performing similarly.
It also opens an avenue for a reduction in the energy usage, as with a new computational architecture, the costly multiplication operations can be eliminated entirely.
In an era where cutting edge models are trained in huge data centers, this represents a vital development within sustainable computing.

Machine learning seems to be increasingly developing towards a direction where models are hand-tuned based on the problem at hand, rather than being brute force approximators.
XPINNs serve as one such specialization focused on the resolution of PDEs, representing an ever complex machine learning landscape.
