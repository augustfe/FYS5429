\subsection{Advection Equation}\label{sec:advection}
\subsubsection{Problem Formulation}
We first consider the one-dimensional linear advection equation, with discontinuous residual $f$.
The problem has a simple analytical solution, serving as a sanity check for whether our implementation of the method has any merit.
The problem is defined by \eqref{eq:advection} below.
\begin{equation}
\begin{cases}
    u_t + \alpha \cdot u_x= 0 &\text{for } (x,t)\in [-1, 1] \times (0, 1] \\ 
    u(x, 0) = u_0(x)= f &\text{for } x \in [-1, 1],
\end{cases}
\label{eq:advection}
\end{equation}
where $\alpha = 0.5$, and the initial condition $f$ is defined by
\begin{equation*}
    f(x) =
    \begin{cases}
        1 &\text{if } x \in [-0.2, 0.2] \\
        0 &\text{else}.
    \end{cases}
\end{equation*}
By the method of characteristics, the analytical solution is
\begin{equation}
    u(x,t) = u_0(x-\alpha t) =
    \begin{cases}
        1 &\text{if } x - \alpha t \in  [-0.2, 0.2], \\
        0 &\text{else}.
    \end{cases}
\end{equation}

Thus, the problem defines a wave travelling rightwards without diffusion with a speed $\alpha$.
The analytical solution also predicts two discontinuities.
For our XPINN formulation, we decompose the domain according to the predicted discontinuities as in \autoref{fig:decomp_advection}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Project1XPINNs/figures/advection/advection_train.pdf}
    \caption{Training points for the advection equation with the domain decomposed into three subdomains.}
    \label{fig:decomp_advection}
\end{figure}

In order to verify that our method is not simply learning the decomposition itself, we also verify that the wave is able to travel across the interfaces by setting $\alpha = -1$ while keeping the same decomposition.
Finally we compare our XPINN model with a single PINN training on the entire domain without any subdomains. 

Our network is composed of $6$ hidden layers, each with $20$ nodes, using $\tanh$ as our hidden activation function and no activation for the output layer. The learning rate is set to $10^{-4}$, with \textsc{Adam} as the optimizer.
We utilize $2000$ points for the interior, and $200$ evenly spread around the boundary and interfaces. 

\subsubsection{Results}
We run the models for 10 000 epochs each.
The resulting predictions are seen in \autoref{fig:advection_xpinn_pred}, closely fitting the true solution.
The absolute errors against the true solution can be seen in \autoref{fig:advection_xpinn_error}, where most of the error is found along the interfaces.
This is unsurprising, given the discontinuities along the boundaries.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{Project1XPINNs/figures/advection/xpinn_predictions.png}
    \caption{XPINN Predictions for the advection equation.}\label{fig:advection_xpinn_pred}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{Project1XPINNs/figures/advection/xpinn_error.png}
    \caption{Absolute error against the true solution, XPINN with $\alpha=0.5$.}\label{fig:advection_xpinn_error}
\end{figure}

The loss per epoch is shown in \autoref{fig:loss_per_advection}.
It is interesting to note how the different sub-PINNs adjust in accordance with each other, wherein the reduction in the loss of one sub-PINN often leads to an increase in the others, while the total loss is reduced.
Do note that the model seems to still be improving after 10 000 epochs, and that the learning rate needs further tuning.
We chose to not optimize this problem fully, as we were more interested in learning more about the behaviour of XPINNs, and seeing if our implementation had any merit.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{Project1XPINNs/figures/advection/advection_losses.pdf}
    \caption{Loss per epoch for the advection equation with an XPINN, with $\alpha=0.5$.}
    \label{fig:loss_per_advection}
\end{figure}

Comparing against how a single PINN on the entire domain performs, we see better and more stable results from running the same number of epochs.
The predicted values are shown in \autoref{fig:single_ad_pred}, with the errors in \autoref{fig:single_ad_error}.
XPINNs then probably do not have a lot of merit for such a simple problem, especially given the simplicity of deriving the analytical solution.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{Project1XPINNs/figures/advection/single_advection_predictions.png}
    \caption{Predicted values with a PINN.}
    \label{fig:single_ad_pred}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{Project1XPINNs/figures/advection/single_advection_error.png}
    \caption{Absolute errors with a PINN.}
    \label{fig:single_ad_error}
\end{figure}

We stress test our model by changing the equation in question by setting $\alpha=-1$, while keeping the same domain decomposition.
This is done in order to ensure that the XPINN is not simply learning the domain decomposition, but rather the behaviour we are expecting.
The wave is now able to traverse across the subdomain decomposition, as in \autoref{fig:alpha-1_pred}, however with worse errors in \autoref{fig:alpha-1_error} than we saw previously.
This is a promising result before moving on to more difficult problems.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{Project1XPINNs/figures/advection/xpinn_stress_predictions.png}
    \caption{XPINN predictions with $\alpha=-1$.}
    \label{fig:alpha-1_pred}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{Project1XPINNs/figures/advection/xpinn_stress_error.png}
    \caption{Absolute error with $\alpha=-1$.}
    \label{fig:alpha-1_error}
\end{figure}

% \vfill\null

\subsection{Poisson Equation}
\subsubsection{Problem Formulation}
In this subsection, we consider a two-dimensional inhomogeneous Poisson equation with residual.
The problem is a second order linear PDE, and is defined as
\begin{align}\label{eq:poisson}
\begin{cases}
    u_{xx}+u_{yy} = f &\text{on } \Omega \\
    u(x,y) = 0 &\text{on } \partial\Omega,
\end{cases}
\end{align}
where $\Omega = [0,1] \times [0,1]$.
For the Poisson equation, we consider both a continuous and a discontinuous residual.

\paragraph{Continuous Residual:}
The first residual is given by
\begin{equation}\label{eq:continuous_poisson}
    f_{\text{cont}}(x,y)= -2\pi^2\sin(\pi x) \sin(\pi y),
\end{equation}
which yields the known analytical solution
\begin{equation*}
    u_{\text{true}}(x,y)=\sin(\pi x) \sin(\pi y),
\end{equation*}
which is displayed in \autoref{fig:exact_smooth}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{Project1XPINNs/figures/Poisson/smooth_Poisson_true.png}
    \caption{The exact analytical solution to the Poisson equation with continuous residual.}
    \label{fig:exact_smooth}
\end{figure}

Our experimental setup follows that of \textcite{m√ºller2023achieving}.
We consider the problem for both a single PINN and for an XPINN decomposition shown in \autoref{fig:decomp_poisson}.
For the PINN, we train using 900 interior points and 120 boundary points distributed uniformly.
For the XPINN, we also add 80 uniformly distributed interface points. Thus, the total amount of training points used for the XPINN exceeds that for the PINN.
We apply unitary weights to the different losses.
All PINNs have the same architecture of a 3-layered neural network with 64 hidden units, and we use the $\tanh$ activation function. 
Both solvers are trained using the \textsc{Adam} optimizer with an exponentially decreasing learning rate.
The initial learning rate is $10^{-3}$.
After $1.5\times 10^4$ steps, we decrease the learning rate by a factor of $10^{-1}$ every $10^4$ steps until a minimum learning rate of $10^{-7}$.
We train both cases for $2\times 10^5$ iterations. For our testing, we consider a uniform grid with 1 000 000 points. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Project1XPINNs/figures/Poisson/poisson_train_points.pdf.pdf}
    \caption{Training points with the XPINN decomposition for the Poisson equation. The PINN is given the same points, without the interface points.}
    \label{fig:decomp_poisson}
\end{figure}

\vspace{-1cm}
\paragraph{Discontinuous Residual:}
The second residual is due \textcite{XPINN_generalize}, where they compare the generalizeability of PINNs and XPINNs.
The residual is given as a discontinuous form
\begin{equation*}\label{eq:discontinuous_poisson}
    f_{disc}(x,y)=
    \begin{cases}
        1 &\text{if} \, (x,y)\in [0.25,0.75]\times[0.25,0.75], \\
        0 &\text{else}.
    \end{cases}
\end{equation*}

The reference solution is computed numerically using the \verb|poissonpy| package \cite{poissonpy} on a uniform test mesh with 1 000 000 points, displayed in \autoref{fig:exact_discontinuous}.
We apply a 9 layer neural network with 20 hidden units, using the $\tanh$ activation function.
Again, we apply 900 internal, 120 boundary and 80 interface points for training the XPINN model, as in \autoref{fig:decomp_poisson}.
Additionally, we apply 20 weights to the boundary conditions for the PINN, and 20 for boundary and interface conditions for our XPINN. 
\textsc{Adam} is again used, and the solvers are run for $2\times 10^5$ iterations. The exponential decay rate is the same as in the continuous experiment, although the minimum learning rate is set as $10^{-5}$.

Note that the paper by \textcite{XPINN_generalize} uses the LBFGS optimizer, however this method is not implemented in our chosen optimization library \verb|optax|.
LBFGS is available in \verb|JAXopt| \cite{jaxopt_implicit_diff}.
However, \verb|JAXopt| is currently being merged into \verb|optax|.
Adapting our implementation to be compatible with \verb|JAXopt| falls outside the scope of this project.

\begin{figure}[!h]
    \centering
    \includegraphics[width = 0.8\linewidth]{Project1XPINNs/figures/Poisson/discrete_Poisson_solution.png}
    \caption{Numerically computed reference solution to the Poisson equation with discontinuous residual.}
    \label{fig:exact_discontinuous}
\end{figure}
% \vfill\null

\subsubsection{Results}
\paragraph{Continuous Residual:}
\autoref{table:cont_poisson} shows the minimum, median, and maximum relative $L^2$ errors of the single PINN and XPINN solvers for the continuous residual Poisson equation using the \textsc{Adam} optimizer, across ten iterations.
Note that the training mesh was re-generated between each iteration.

% \vfill\null

\autoref{fig:rel_l2_smooth_poisson} displays the $L^2$ error evolution against the iterations.
From the figure, we see that the $L^2$ error seems to stabilize.
Although of similar orders of magnitude, we see that the PINN model frequently outperforms our XPINN discretization.
This is as expected by the argument of \textcite{XPINN_generalize}, as the domain decomposition does not reduce the target function complexity in the subdomains, whereas each subdomain trains on fewer points than the PINN. 

\begin{table}[h]
\caption{Median, minimum and maximum of the relative $L^2$-errors for the Poisson equation with continuous residual achieved by the PINN and XPINN.}
\centering
\begin{tabular}{r|c|c|c}\toprule
     & Median & Minimum & Maximum \\
    \colrule
    PINN & $1.37\cdot 10^{-3}$ &  $8.4 \cdot 10^{-4}$ & $2.16 \cdot 10^{-3}$ \\
    XPINN & $2.82\cdot 10^{-3} $ &  $1.23 \cdot 10^{-3}$ & $5.42 \cdot 10^{-3}$ \\
    \botrule
\end{tabular}
\label{table:cont_poisson}
\vspace{-0.5cm}
\end{table}
\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.8\linewidth]{Project1XPINNs/figures/Poisson/smooth_l2_error_evolution.pdf}
    \caption{Relative $L^2$ error with median and interquartile range highlighted, for continuous residual.}
    \label{fig:rel_l2_smooth_poisson}
\end{figure}

The PINN and XPINN predictions are shown in \autoref{fig:pinn_smooth_pred} and \autoref{fig:xpinn_smooth_pred}, and although they are hard to distinguish visually, the errors displayed in \autoref{fig:pinn_smooth_error} and \autoref{fig:xpinn_smooth_error} tell a different story.
Note that the error figures are created with the same colorbar, in order to be comparable.
We note that the significant sources of error for the PINN appear to be surrounding the boundaries.
The XPINN solution also shows boundary error, but the interface error seems to dominate.
Optimizing for the different weighting hyper-parameters might lower the interface error, potentially at the cost of increased boundary error.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Project1XPINNs/figures/Poisson/smooth_single_Poisson_solution.png}
    \caption{PINN prediction with continuous residual.}
    \label{fig:pinn_smooth_pred}
\end{figure}
% \vspace{-1}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Project1XPINNs/figures/Poisson/smooth_xpinn_Poisson_solution.png}
    \caption{XPINN prediction with continuous residual.}
    \label{fig:xpinn_smooth_pred}
\end{figure}

% \vfill\null

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Project1XPINNs/figures/Poisson/smooth_single_Poisson_error.png}
    \caption{PINN absolute error with continuous residual.}
    \label{fig:pinn_smooth_error}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Project1XPINNs/figures/Poisson/smooth_xpinn_Poisson_error.png}
    \caption{XPINN absolute error with continuous residual.}
    \label{fig:xpinn_smooth_error}
\end{figure}

% \vfill\null

\paragraph{Discontinuous Residual:}
\autoref{fig:pinn_disc_pred} and \autoref{fig:xpinn_disc_pred} display the PINN and XPINN solutions.
Compared with the continuous residual, it is now much clearer where the subdomain boundaries are.
Interestingly, the outer sub-PINN visually looks similar to the solution of the PINN.
The error are shown in \autoref{fig:pinn_disc_error} and \autoref{fig:xpinn_disc_error}, this time with different colorbars as the maximum error of the XPINN is significantly higher than that of the PINN.
Both solvers seem to perform worse on this problem. Notably, the XPINN solver appears to perform significantly worse, which is according to expectations \cite{XPINN_generalize}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Project1XPINNs/figures/Poisson/discrete_single_Poisson_solution.pdf.png}
    \caption{PINN prediction with discontinuous residual.}
    \label{fig:pinn_disc_pred}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Project1XPINNs/figures/Poisson/new_exp/discrete_xpinn_Poisson_solution.png}
    \caption{XPINN prediction with continuous residual.}
    \label{fig:xpinn_disc_pred}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Project1XPINNs/figures/Poisson/discrete_single_Poisson_error.pdf.png}
    \caption{PINN absolute error with discontinuous residual.}
    \label{fig:pinn_disc_error}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Project1XPINNs/figures/Poisson/new_exp/discrete_xpinn_Poisson_error.png}
    \caption{XPINN absolute error with discontinuous residual.}
    \label{fig:xpinn_disc_error}
\end{figure}

Wondering whether this discrepancy is due a lack of training, we investigated the loss per epoch, shown in \autoref{fig:xpinn_disc_loss}.
As we see, the loss only sees marginal improvements after 125 000 iterations.
The same is seen with the relative $L^2$ error in \autoref{fig:rel_l2_discrete_poisson}, wherein the PINNs stabilize, while the XPINNs remains higher.
We therefore believe this is partly due to the reduced number of training points.
The points are drawn from a uniform distribution, and as the center subdomain is $[0.25, 0.75] \times [0.25, 0.75]$, it receives on average just $25\%$ of the total points.
In addition, given the symmetric nature of the problem, one can argue for whether evenly spaced points might outperform our choice of uniformly random points.

The relative $L^2$ errors are also summarized in \autoref{table:discont_poisson}.
The XPINNs give a median error about an order of magnitude higher than that of the PINN over 10 runs.
These results are consistent with what was found in \cite{XPINN_generalize}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Project1XPINNs/figures/Poisson/new_exp/discrete_xpinn_Poisson_losses.pdf}
    \caption{Loss per epoch for XPINN on the Poisson equation with discontinuous residual.}
    \label{fig:xpinn_disc_loss}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width = \linewidth]{Project1XPINNs/figures/Poisson/discrete_l2_error_evolution.pdf}
    \caption{Relative $L^2$ error with median and interquartile range highlighted, for PINN and XPINN with discontinuous residual.}
    \label{fig:rel_l2_discrete_poisson}
\end{figure}

\begin{table}[h!]
\caption{Median, minimum and maximum of the relative $L^2$-errors for the Poisson equation with discontinuous residual achieved by the PINN and XPINN.}
\centering
\begin{tabular}{r|c|c|c}\toprule
     & Median & Minimum & Maximum \\
    \colrule
    PINN & $3.73 \cdot 10^{-2}$ & $3.11 \cdot 10^{-2}$ & $6.05 \cdot 10^{-2}$ \\
    XPINN & $4.69 \cdot 10^{-1} $ &  $2.36 \cdot 10^{-1}$ & $1.00 \cdot 10^{0}$ \\
    \botrule
\end{tabular}
\label{table:discont_poisson}
% \vspace{-0.5cm}
\end{table}
\vfill\null

\subsection{Navier-Stokes Equation}
\subsubsection{Problem Formulation}
Finally, we consider the steady Navier-Stokes equation at Reynolds number $Re = 20$ as described in the FEATFLOW benchmark suite \cite{DFG}.
The problem is a simulation of incompressible 2D fluid flow past a cylinder in a rectangular domain.
The steady, incompressible Navier Stokes equation can be written as
\begin{equation}\label{eq:NS}
    -\nu \nabla^2 \mathbf{u} + (\mathbf{u} \cdot \nabla)\mathbf{u} + \nabla p = \mathbf{0}, \quad \nabla \cdot \mathbf{u} = 0,
\end{equation}
where $\nu = 0.001$.
In our notation, we will isolate the $x$- and $y$-flow as $u$ and $v$ respectively, writing $\mathbf{u} = (u,v)^\intercal$.
From this, we can express the PDEs for the second-order spatial derivatives, as well as the equation for zero convergence due to incompressibility as
\begin{equation}
\begin{cases}
-\nu(u_{xx} + u_{yy})+uu_x+vu_y+p_x = 0, \\
-\nu(v_{xx} + v_{yy})+uv_x+vv_y+p_y = 0, \\
u_x + v_y = 0.
\end{cases}
\end{equation}

The domain of the problem is given by
\vspace{-1mm}
\begin{align*}
    \Omega = [0,2.2] \times [0,0.41] \setminus B_r(0.2,0.2), \hspace{4mm} r = 0.05
\end{align*}

In order to impose the zero divergence condition, we consider the stream function $\psi$ characterizing incompressible flow. $u$ and $v$ can then be obtained through $u=\psi_y$ and $v=-\psi_x$. Our network $\mathcal{N}_\theta$ thus predicts the stream function $\psi$ and the pressure $p$, with
\begin{equation}
    \mathcal{N}_\theta : \mathbb{R}^2 \to \mathbb{R}^2, \quad (x,y)\to (\psi, p).
\end{equation}


This problem includes three different boundary conditions.
First, we have the boundary for the upper and lower walls, as well as around the edge of the cylinder.
In the DFG benchmark 2D-1 \cite{DFG}, the lower and upper walls are notated as $\Gamma_1 = [0,2.2]\times 0$ and $\Gamma_3 = [0,2.2]\times 0.41$ respectively, and the boundary as $S=\partial B_r(0.2,0.2)$.
On these walls, we enforce impermeability and the no-slip conditions, leading to zero-flow on these boundaries. 
The no-slip boundary condition is defined as
\begin{align*}
    u_{|\Gamma_1} = u_{|\Gamma_3} = u_{|S} &= 0, \\
    v_{|\Gamma_1} = v_{|\Gamma_3} = v_{|S} &= 0.
\end{align*}
On the left boundary we enforce polynomial inflow.
The left boundary is $\Gamma_2 = 0\times [0,0.41]$ and the polynomial inflow is given by
\begin{equation*}
    u=\frac{4Uy(0.41-y)}{0.41^2}, \quad v=0
\end{equation*}
where $U=0.3$, and the mean velocity inlet is $U_{\text{mean}} = 0.2$.
Finally, on the right boundary we have the do-nothing boundary condition.
The boundary is $\Gamma_4=2.2\times[0,0.41]$, and the condition is given by
\begin{equation*}
    \nu u_x - p = 0, \quad \nu u_y = 0.
\end{equation*}
Hence, we confirms our target Reynolds number using the cylinder diameter as the characteristic length $L=0.1$. $U_{\text{mean}}$ is used as the characteristic velocity. The Reynolds number becomes:
\begin{equation*}
    Re = \frac {U_{\text{mean}} L} \nu = \frac {0.2 \cdot 0.1} {0.001} = 20 
\end{equation*}

\subsubsection{Results}
\begin{comment}
We were unable to produce satisfying results for Navier-Stokes.
This is mainly due the large number of hyper-parameters, and the high computational complexity of the derivatives involved, which restricted our ability to tune as we did not have access to the proper resources required for such tasks.
We are exceedingly close to including the produced results here, but were unable to include them at this time.
\end{comment}



\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Project1XPINNs/figures/NavierStokes/NoDecomp/ND_10000_iter_30x25/losses/PINN_losses.pdf}
    \caption{Loss per epoch for PINN on the Navier-Stokes equation}
    \label{fig:NS_PINN_loss}
\end{figure}


\par
Achieving qualitative results required some parameter tuning, of weights for losses, model architecture and decomposition for the XPINN. This was not carried out extensively, but rather through a heuristic approach of trial and error. Given this, it should be recognized that there may be potential to achieve better results. 

The results we chose to present where produced by a PINN which incorporated 25 hidden layers, each with 30 nodes, sticking with the tanh activation function with no weighting on the loss functions. While our XPINN consisted of a decompositon in to two domains given by
\begin{align*}
    \Omega_1 =& [0, 0.8] \times [0,0.41] \setminus B_r(0.2,0.2), \hspace{4mm} r = 0.05\\
    \Omega_2 =& [0.8, 2.2] \times [0, 0.41]
\end{align*}

With 25 and 10 hidden layers with 30 nodes each respectively for $\Omega_1$ and $\omega2$.
\par
The XPINN struggled with the inferface, so in order to aid it's performance we imposed neumann interface conditions on $\psi$ .  That is continuity conditions on the flow field.

\begin{align*}
    \frac{2}{N_{I^{i,j}}} \sum_{(x,y) \in {I^{i,j}}}^n || \nabla \psi_i(x,y)  - \nabla \psi_j(x,y)||^2
\end{align*}
Where $I^{i,j}$ represents interface between PINNs i and j.

Our results are consistent with our previous experiments: our PINN model outperformed the XPINN.

Figure \ref{fig:NS_PINN_loss} illustrates the loss per epoch for our PINN model. The model architecture incorporated 25 hidden layers, each with 30 nodes, sticking with the tanh activation function. Although there was a significant decrease in loss, it did not plateau to the same extent as with our Poisson or advection models. This could suggest that additional training might yield further improvements in performance.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Project1XPINNs/figures/NavierStokes/NoDecomp/ND_10000_iter_30x25/solution/flow_magnitude_no_decomp.png}
    \caption{PINN Velocity Magnitude}
    \label{fig:NS_PINN_FlowMag}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Project1XPINNs/figures/NavierStokes/NoDecomp/ND_10000_iter_30x25/solution/pressure_no_decomp.png}
    \caption{PINN Pressure}
    \label{fig:NS_PINN_Pressure}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Project1XPINNs/figures/NavierStokes/NoDecomp/ND_10000_iter_30x25/solution/streamfunc_no_decomp.png}
    \caption{PINN Streamfunction}
    \label{fig:NS_PINN_Streamfunc}
\end{figure}

Having said that, our PINN successfully replicated the flow plots from the DFG benchmark  \cite{DFG}, as illustrated in Figures \ref{fig:NS_PINN_FlowMag} through \ref{fig:NS_PINN_Streamfunc}. It should be noted on comparison with the DFG results that somewhat different colormaps where used, which may make the plots appear more distinct.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Project1XPINNs/figures/NavierStokes/TwoBoxDecomp/TB_10000_iter_right_emphasis/losses/TwoBox_decomp_losses.pdf}
    \caption{Loss per epoch for XPINN on the Navier-Stokes equation}
    \label{fig:NS_XPINN_losses}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Project1XPINNs/figures/NavierStokes/TwoBoxDecomp/TB_10000_iter_right_emphasis/solution/flow_magnitude_two_box.png}
    \caption{XPINN Velocity Magnitude}
    \label{fig:NS_XPINN_flow_mag}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Project1XPINNs/figures/NavierStokes/TwoBoxDecomp/TB_10000_iter_right_emphasis/solution/pressure_two_box.png}
    \title{XPINN Velcoity Magnitude}
    \label{fig:NS_XPINN_pressure}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Project1XPINNs/figures/NavierStokes/TwoBoxDecomp/TB_10000_iter_right_emphasis/solution/streamfunc_two_box.png}
    \title{XPINN Velcoity Magnitude}
    \label{fig:NS_XPINN_flow_mag}
\end{figure}





