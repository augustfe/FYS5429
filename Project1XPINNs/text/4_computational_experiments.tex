\subsection{Advection Equation}\label{sec:advection}
\subsubsection{Problem Formulation}
We first consider the one-dimensional linear advection equation, with discontinuous residual $f$.
The problem has a simple analytical solution, serving as a sanity check for whether our implementation of the method has any merit.
The problem is defined by \eqref{eq:advection} below.
\begin{equation}
\begin{cases}
    u_t + \alpha \cdot u_x= 0 &\text{for } (x,t)\in [-1, 1] \times (0, 1] \\ 
    u(x, 0) = u_0(x)= f &\text{for } x \in [-1, 1],
\end{cases}
\label{eq:advection}
\end{equation}
where $\alpha = 0.5$, and the initial condition $f$ is defined by
\begin{equation*}
    f(x) =
    \begin{cases}
        1 &\text{if} \quad x \in [-0.2, 0.2] \\
        0 &\text{else}.
    \end{cases}
\end{equation*}

By the method of characteristics, the analytical solution is:
\begin{equation}
    u(x,t) = u_0(x-\alpha t)=
    \begin{cases}
        1 &\text{if } \quad x \in [-0.2 + \alpha t, 0.2 + \alpha t] \\
        0 &\text{else}.
    \end{cases}
\end{equation}

Thus, the problem defines a wave travelling rightwards without diffusion with a speed $\alpha$. The analytical solution also predicts two discontinuities. For our XPINN formulation, we decompose the domain according to the predicted discontinuities as in \autoref{fig:decomp_advection} 
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.48\textwidth]{Project1XPINNs/figures/advection/advection_decomp.png}
    \label{subfig:single}
    \caption{Illustration of advection domain decomposition.}
    \label{fig:decomp_advection}
\end{figure}

In order to verify that our method is not simply learning the decomposition itself, we also verify that the wave is able to travel across the interfaces by setting $\alpha = -1$ while keeping the same decomposition.

Finally we compare our XPINN model with a single PINN training on the entire domain without any subdomains. 

Our network is composed of $6$ hidden layers, each with $20$ nodes, using $\tanh$ as our hidden activation function and no activation for the output layer. The learning rate is set to $10^{-4}$.
We utilize $2000$ points for the interior, and $200$ evenly spread around the boundary and interfaces. 

\subsubsection{Results}

\begin{figure*}
\subfloat[predictions]{
\includegraphics[width=0.48 \textwidth]{Project1XPINNs/figures/advection/0.5_predictions.png}
}
\label{subfig:0.5_predictions}
\subfloat[error]{
\includegraphics[width=0.48 \textwidth]{Project1XPINNs/figures/advection/0.5_error.png}
}
\subfloat[loss]{
\includegraphics[width=0.48 \textwidth]{Project1XPINNs/figures/advection/Loss per Pinn over 10000 epochs, alpha=0.5_10000.pdf}
}
\label{fig:0.5_advection}
\end{figure*}


\begin{figure*}
\subfloat[predictions]{
\includegraphics[width=0.48 \textwidth]{Project1XPINNs/figures/advection/-1_predictions.png}
}
\label{subfig:0.5_predictions}
\subfloat[error]{
\includegraphics[width=0.48 \textwidth]{Project1XPINNs/figures/advection/-1_error.png}
}
\subfloat[loss]{
\includegraphics[width=0.48 \textwidth]{Project1XPINNs/figures/advection/Loss_per_Pinn_over_10000_epochs,_alpha=-1.pdf}
}
\label{fig:-1_advection}
\end{figure*}

\begin{figure*}
\subfloat[predictions]{
\includegraphics[width=0.48 \textwidth]{Project1XPINNs/figures/advection/single_0.5_predictions.png}
}
\label{subfig:0.5_predictions}
\subfloat[error]{
\includegraphics[width=0.48 \textwidth]{Project1XPINNs/figures/advection/single_0.5_error.png}
}
\subfloat[loss]{
\includegraphics[width=0.48 \textwidth]{Project1XPINNs/figures/advection/Single_PINN.pdf}
}
\label{fig:pinn_advection}
\end{figure*}

% halloen!
%%%%% TJOHEI

\subsection{Poisson Equation}
\subsubsection{Problem Formulation}
In this subsection, we consider a two-dimensional Poisson equation with residual discontinuity. The problem is a second order linear PDE, and is defined as:
\begin{align}\label{eq:poisson}
\begin{cases}
    u_{xx}+u_{yy} &= f \text{ on } \Omega \\
    u(x,y) &= 0 \text{ on } \partial\Omega,
\end{cases}
\end{align}
where $\Omega = (0,1) \times (0,1)$. For the Poisson problem, we consider two residuals. 
\paragraph{Continuous Residual:}
The first residual is given by:
\begin{equation}
    f_{cont}(x,y)= -2\pi^2\sin(\pi x) \sin(\pi y),
    \label{eq:continuous_poisson}
\end{equation}
which yields a known analytical solution:
\begin{equation*}
    u_{true}(x,y)=\sin(\pi x) \sin(\pi y),
\end{equation*}
which is displayed in \autoref{fig:exact_smooth}.

\begin{figure}
    \centering
    \includegraphics[width = \linewidth]{Project1XPINNs/figures/Poisson/Exact_Smooth.png}
    \caption{The exact analytical solution to the Poisson problem with continuous residual.}
    \label{fig:exact_smooth}
\end{figure}
Our experimental setup follows that of \textcite{m√ºller2023achieving}.
We consider the problem for both a single PINN and for an XPINN decomposition shown in \autoref{fig:decomp_poisson}.
For the PINN, we train using 900 interior points and 120 boundary points distributed uniformly.
For the XPINN, we also add 80 uniformly distributed interface points. Thus, the total amount of training points used for the XPINN exceeds that for the PINN. We apply unitary weights to the boundary and interface losses.
Both solvers are trained using the \textsc{Adam} optimizer with an exponentially decreasing learning rate.
The initial learning rate is $10^{-3}$.
After $1.5\times 10^4$ steps, we decrease the learning rate by a factor of $10^{-1}$ every $10^4$ steps until a minimum learning rate of $10^{-7}$.
We train both cases for $2\times 10^5$ iterations. For our testing, we consider a uniform grid with 1,000,000 points. 

\begin{figure*}[t]
    % \centering
    \subfloat[PINN]{
    \includegraphics[width=0.48\textwidth]{Project1XPINNs/figures/Poisson/single_pinn_decomp.pdf}
    \label{subfig:single}
    }
    \hfill
    \subfloat[XPINN]{
    \includegraphics[width=0.48\textwidth]{Project1XPINNs/figures/Poisson/xpinn_decomp.pdf}
    \label{subfig:xpinn_decomp}
    }
    \caption{Example training points for the Poisson problem for a single PINN across the domain (left) and an XPINN where we have on PINN confined in the square $[0.25, 0.75]\times [0.25, 0.75]$ and the other for the remaining points. In both cases, we define 900 internal points and 120 boundary points. For the XPINN, we define an additional 80 interface points.}
    \label{fig:decomp_poisson}
\end{figure*}

\paragraph{Discrete Residual:}
The second residual is due \cite{XPINN_generalize}, where they compare the generalizeability of PINNs and XPINNs. The residual is given as a discrete form:
\begin{equation*}
    f_{disc}(x,y)=
    \begin{cases}
        1 &\text{if} \, (x,y)\in [0.25,0.75]\times[0.25,0.75], \\
        0 &\text{else}.
        \label{eq:discrete_poisson}
    \end{cases}
\end{equation*}
The reference solution is computed numerically using the \textit{poissonpy} package by \cite{poissonpy} on the uniform test mesh with 1 000 000 points. \autoref{fig:exact_discrete} displays the exact solution. Again, we apply 900 internal training points, 120 boundary points, and 80 interface points for the XPINN model. \textsc{Adam} is again used with the same exponential decay rate, and the solvers are run for $2\times 10^5$ iterations. Note that the paper by \textcite{XPINN_generalize} uses the LBFGS optimizer. However, this is currently unavailable in optax as it is under development.

\begin{figure}
    \centering
    \includegraphics{Project1XPINNs/figures/Poisson/Exact_Discrete.png}
    \caption{Numerically computed solution to the Poisson problem with discrete residual}
    \label{fig:exact_discrete}
\end{figure}


Using our XPINN methodology, we aim to reproduce the results presented by \textcite{XPINN_generalize}.

\subsubsection{Results}
\paragraph{Continuous Residual:}
\autoref{table:cont_poisson} shows the minimum, median, and maximum relative $L^2$ errors of the single PINN and XPINN solvers for the continuous residual Poisson equation using the \textsc{Adam} optimizer. \autoref{fig:rel_l2_smooth_poisson} also displays the $L^2$ error evolution against the iterations. Note that the mesh was re-generated between each iteration. Although of similar orders of magnitude, we see that the PINN model frequently outperforms our XPINN discretization. This is as expected by the argument of \textcite{XPINN_generalize}, as the domain decomposition does not reduce the target function
complexity in the subdomains, whereas each subdomain trains on fewer points than the PINN. 

\begin{table}[h]
\caption{Median, minimum and maximum of the relative $L^2$-errors for the Poisson equation with continuous residual achieved by the PINN and XPINN.}
    \centering
    \begin{tabularx}{\linewidth}{|X|XXX|}
    \hline
     & Median & Minimum & Maximum
    \\
    \hline
    PINN &$1.37\cdot 10^{-3}$ &  $8.4 \cdot 10^{-4}$&$2.16 \cdot 10^{-3}$ 
    \\
    XPINN &$2.82\cdot 10^{-3}$ &  $1.23 \cdot 10^{-3}$&$5.42 \cdot 10^{-3}$
    \\
    \hline
    \end{tabularx}
    \label{table:cont_poisson}
\end{table}
\begin{figure}[h!]
    \centering
    \includegraphics[width = \linewidth]{Project1XPINNs/figures/Poisson/Relative_L2_smooth_Adam.png}
    \caption{}
    \label{fig:rel_l2_smooth_poisson}
\end{figure}

The PINN and XPINN solution, alongside their error, is also displayed in figure \autoref{fig:poisson_smooth}
\begin{figure*}
\subfloat[PINN prediction]{
\includegraphics[width=0.48 \textwidth]{Project1XPINNs/figures/Poisson/PINN_Smooth.png}
}
\subfloat[XPINN prediction]{
\includegraphics[width=0.48 \textwidth]{Project1XPINNs/figures/Poisson/XPINN_Smooth.png}
}

\subfloat[PINN error]{
\includegraphics[width=0.48 \textwidth]{Project1XPINNs/figures/Poisson/PINN_error_Smooth.png}
}
\subfloat[XPINN error]{
\includegraphics[width=0.48 \textwidth]{Project1XPINNs/figures/Poisson/XPINN_error_Smooth.png}
}
\label{fig:poisson_smooth}
\end{figure*}

\subsection{Navier-Stokes Equation}
\subsubsection{Problem Formulation}
Finally, we consider the Navier Stokes problem as described in \textcolor{red}{cite benchmark}. This problem is a simulation of an incompressible 2D fluid flow hitting a cylinder in a rectangle. In the DFG benchmark 2D-1, the PDE conditions are given by:
\begin{equation}
    -\nu \nabla^2 \mathbf{u} + (\mathbf{u} \cdot \nabla)\mathbf{u} + \nabla p = \mathbf{0}, \hspace{3mm} \nabla \cdot \mathbf{u} = 0
\end{equation}
where $\nu = 0.001$. In our notation, we will isolate the x- and y-flow as $u$ and $v$ respectively. From this, we can express the PDEs for the second-order spatial derivatives, as well as the equation for zero convergence due to incompressibility as
\begin{equation}
\begin{cases}
-\nu(u_{xx} + u_{yy})+uu_x+vu_y+p_x = 0 \\
-\nu(v_{xx} + v_{yy})+uv_x+vv_y+p_y = 0 \\
u_x + v_y = 0
\end{cases}
\end{equation}
In order to impose the zero divergence condition, we let our network $\mathcal{N}_\theta$ predict both the stream function $\psi$ and the pressure $p$.
\begin{equation}
    \mathcal{N}_\theta : \mathbb{R}^3 \to \mathbb{R}^2, \quad (x,y,t)\to (\psi, p)
\end{equation}
We obtain $u$ and $v$ from the equations $u=\psi_y$ and $v=-\psi_x$. \\
This problem includes three different boundary conditions. First, we have the boundary for the upper and lower walls, as well as around the edge of the cylinder. On these walls, we require the flow to be exactly zero. In the DFG benchmark 2D-1 \textcolor{red}{cite benchmark}, the lower and upper walls are notated as $\Gamma_1 = [0,2.2]\times 0$ and $\Gamma_3 = [0,2.2]\times 0.41$ respectively, and the boundary as $S=\partial B_r(0.2,0.2)$. The no-slip boundary condition is defined as
\begin{align*}
    u_{|\Gamma_1} = u_{|\Gamma_3} = u_{|S} &= 0 \\
    v_{|\Gamma_1} = v_{|\Gamma_3} = v_{|S} &= 0
\end{align*}
On the left boundary we enforce polynomial inflow. The left boundary is $\Gamma_2 = 0\times [0,0.41]$ and the polynomial inflow is given by
\begin{equation*}
    u=\frac{4Uy(0.41-y)}{0.41^2}, \quad v=0
\end{equation*}
where $U=0.3$
Finally, on the right boundary we have the do-nothing boundary condition. The boundary is $\Gamma_4=2.2\times[0,0.41]$, and the condition is given by
\begin{equation*}
    \nu u_x - p = 0, \quad \nu u_y = 0
\end{equation*}
\subsubsection{Results}