%%%%%%%%%%%%%%%% README %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Place the BIBTeX entry in the right category 
% 
% BIBTeX labels:  {SurnameYear}, as in Albertsson2018
% Capitalized words in titles must be placed in {...}, as in:
%                       title = {Beyond the proton drip line: {Bayesian} analysis}
% Journal abbreviations:should be done according to the ISO4 standard:
%                       https://academic-accelerator.com/Journal-Abbreviation/System
% doi - include if possible
% url - include if possible
% do not include: abstract; eprint data, report numbers  for published papers;

%%%%%%%%%%%%%%%% XPINN %%%%%%%%%%%%%%%%

%%% Numeric Poisson solver
@misc{poissonpy,
    author = {Brian Chao},
    title = {poissonpy},
    year = {2022},
    url = {https://github.com/bchao1/poissonpy},
    commit = {81b1c30c30e13052bc18f3a4e693a2295301e213}
}

%%% DFG 2D-1 benchmark
@manual{DFG,
  title        = {{DFG} flow around cylinder benchmark {2D-1}, laminar case {Re=20}},
  author       = {{FEATFLOW}},
  year         = {2011},
  month        = {June},
  address      = {Vogelpothsweg 87, Dortmund},
  note         = {Available at \url{https://wwwold.mathematik.tu-dortmund.de/~featflow/en/benchmarks/cfdbenchmarking/flow/dfg_benchmark1_re20.html}, accessed 31. march 2024},
  organization = {Lehrstuhl III, Angewandte Mathematik und Numerik, Technische Universität Dortmund},
}

% Universal Approximation Theorem
@article{Cybenko1989ApproximationBS,
    title={Approximation by superpositions of a sigmoidal function},
    author={George V. Cybenko},
    journal={Mathematics of Control, Signals and Systems},
    year={1989},
    volume={2},
    pages={303-314},
    url={https://api.semanticscholar.org/CorpusID:3958369}
}

% Backpropogation
@article{Backpropogation1986,
  title={Learning representations by back-propagating errors},
  author={David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  journal={Nature},
  year={1986},
  volume={323},
  pages={533-536},
  url={https://api.semanticscholar.org/CorpusID:205001834}
}

% Automatic differentiation
@misc{autodiff,
    title = {Automatic Differentiation in Neural Nets},
    url = {https://mehta-rohan.com/writings/blog_posts/autodiff.html},
    author = {Rohan Mehta},
    year = {2021},
    note = {Accessed on March 22, 2024}
}


@InProceedings{Glorot2010UnderstandingTD,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 9,
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/glorot10a.html},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

% Llama
@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% 1.58bit models
@misc{ma2024era,
      title={The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits}, 
      author={Shuming Ma and Hongyu Wang and Lingxiao Ma and Lei Wang and Wenhui Wang and Shaohan Huang and Li Dong and Ruiping Wang and Jilong Xue and Furu Wei},
      year={2024},
      eprint={2402.17764},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gpt4-parameters,
    title = {{GPT-4} architecture, datasets, costs and more leaked},
    author = {Maximilian Schreiner},
    url = {https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/},
    year = {2023},
    note = {Accessed on March 24, 2024}
}

% Natural Gradients
@misc{müller2023achieving,
  author = {Johannes M{\"u}ller and Marius Zeinhofer},
  title = {Achieving High Accuracy with PINNs via Energy Natural Gradient Descent},
  abstract = {We propose energy natural gradient descent, a natural gradient method with respect to a Hessian-induced Riemannian metric as an optimization algorithm for physics-informed neural networks (PINNs) and the deep Ritz method. As a main motivation we show that the update direction in function space resulting from the energy natural gradient corresponds to the Newton direction modulo an orthogonal projection onto the model{\textquoteright}s tangent space. We demonstrate experimentally that energy natural gradient descent yields highly accurate solutions with errors several orders of magnitude smaller than what is obtained when training PINNs with standard optimizers like gradient descent, Adam or BFGS, even when those are allowed significantly more computation time. We show that the approach can be combined with deterministic and stochastic discretizations of the integral terms and with deep networks allowing for an application in higher dimensional settings.},
  year = {2023},
  journal = {International Conference on Machine Learning (ICML)},
  publisher = {PMLR},
}

% Learning rate free
@misc{sharrock2023learning,
      title={Learning Rate Free Sampling in Constrained Domains}, 
      author={Louis Sharrock and Lester Mackey and Christopher Nemeth},
      year={2023},
      eprint={2305.14943},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


%XPINNs
@article{Jagtap2020ExtendedPN,
    title={Extended {P}hysics-informed {N}eural {N}etworks ({XPINN}s): A Generalized Space-Time Domain Decomposition based Deep Learning Framework for Nonlinear Partial Differential Equations},
    author={Ameya Dilip Jagtap and George E. Karniadakis},
    journal={Communications in Computational Physics},
    year={2020},
    url={https://api.semanticscholar.org/CorpusID:229083388}
}

%XPINN follow-up: When do XPINNs generalize
@article{XPINN_generalize,
    author = {Hu, Zheyuan and Jagtap, Ameya D. and Karniadakis, George Em and Kawaguchi, Kenji},
    title = {When Do Extended Physics-Informed Neural Networks (XPINNs) Improve Generalization?},
    journal = {SIAM Journal on Scientific Computing},
    volume = {44},
    number = {5},
    pages = {A3158-A3182},
    year = {2022},
    doi = {10.1137/21M1447039},
    URL = {https://doi.org/10.1137/21M1447039},
    eprint = {https://doi.org/10.1137/21M1447039},
    abstract = { Physics-informed neural networks (PINNs) have become a popular choice for solving high-dimensional partial differential equations (PDEs) due to their excellent approximation power and generalization ability. Recently, extended PINNs (XPINNs) based on domain decomposition methods have attracted considerable attention due to their effectiveness in modeling multiscale and multiphysics problems and their parallelization. However, theoretical understanding of their convergence and generalization properties remains unexplored. In this study, we take an initial step towards understanding how and when XPINNs outperform PINNs. Specifically, for general multilayer PINNs and XPINNs, we first provide a prior generalization bound via the complexity of the target functions in the PDE problem and a posterior generalization bound via the posterior matrix norms of the networks after optimization. Moreover, based on our bounds, we analyze the conditions under which XPINNs improve generalization. Concretely, our theory shows that the key building block of XPINN, namely, the domain decomposition, introduces a tradeoff for generalization. On the one hand, XPINNs decompose the complex PDE solution into several simple parts, which decreases the complexity needed to learn each part and boosts generalization. On the other hand, decomposition leads to less training data being available in each subdomain, and hence such a model is typically prone to overfitting and may become less generalizable. Empirically, we choose five PDEs to show when XPINNs perform better than, similar to, or worse than PINNs, hence demonstrating and justifying our new theory. }
}

%PINNs
@article{RAISSI2019686,
    title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
    journal = {Journal of Computational Physics},
    volume = {378},
    pages = {686-707},
    year = {2019},
    issn = {0021-9991},
    doi = {https://doi.org/10.1016/j.jcp.2018.10.045},
    url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
    author = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
    keywords = {Data-driven scientific computing, Machine learning, Predictive modeling, Runge–Kutta methods, Nonlinear dynamics},
    abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.}
}

%cPINNs, XPINNs father
@ARTICLE{2020CMAME.36513028J,
       author = {{Jagtap}, Ameya D. and {Kharazmi}, Ehsan and {Karniadakis}, George Em},
        title = "{Conservative physics-informed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems}",
      journal = {Computer Methods in Applied Mechanics and Engineering},
     keywords = {cPINN, Mortar PINN, Domain decomposition, Machine learning, Conservation laws, Inverse problems},
         year = 2020,
        month = jun,
       volume = {365},
          eid = {113028},
        pages = {113028},
          doi = {10.1016/j.cma.2020.113028},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020CMAME.36513028J},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



%APINNs, XPINNs son
@article{Hu_2023,
   title={Augmented Physics-Informed Neural Networks (APINNs): A gating network-based soft domain decomposition methodology},
   volume={126},
   ISSN={0952-1976},
   url={http://dx.doi.org/10.1016/j.engappai.2023.107183},
   DOI={10.1016/j.engappai.2023.107183},
   journal={Engineering Applications of Artificial Intelligence},
   publisher={Elsevier BV},
   author={Hu, Zheyuan and Jagtap, Ameya D. and Karniadakis, George Em and Kawaguchi, Kenji},
   year={2023},
   month=nov, pages={107183} }

%ADAM algorithm (AKA BADam)
@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{reddi2019convergence,
      title={On the Convergence of {A}dam and Beyond}, 
      author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
      year={2019},
      eprint={1904.09237},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{deepmind2020jax,
  title = {The {D}eep{M}ind {JAX} {E}cosystem},
  author = {DeepMind and Babuschkin, Igor and Baumli, Kate and Bell, Alison and Bhupatiraju, Surya and Bruce, Jake and Buchlovsky, Peter and Budden, David and Cai, Trevor and Clark, Aidan and Danihelka, Ivo and Dedieu, Antoine and Fantacci, Claudio and Godwin, Jonathan and Jones, Chris and Hemsley, Ross and Hennigan, Tom and Hessel, Matteo and Hou, Shaobo and Kapturowski, Steven and Keck, Thomas and Kemaev, Iurii and King, Michael and Kunesch, Markus and Martens, Lena and Merzic, Hamza and Mikulik, Vladimir and Norman, Tamara and Papamakarios, George and Quan, John and Ring, Roman and Ruiz, Francisco and Sanchez, Alvaro and Sartran, Laurent and Schneider, Rosalia and Sezener, Eren and Spencer, Stephen and Srinivasan, Srivatsan and Stanojevi\'{c}, Milo\v{s} and Stokowiec, Wojciech and Wang, Luyu and Zhou, Guangyao and Viola, Fabio},
  url = {http://github.com/google-deepmind},
  year = {2020},
}

@misc{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@misc{StanfordXavier,
    author = {Andrew Ng},
    title = {Section 4 (Week 4): {X}avier {I}nitialization and {R}egularization},
    url = {https://cs230.stanford.edu/section/4/},
    year = {2023},
    note = {Accessed on March 24, 2024},
}

@misc{tanhXavier,
    author = {Kian Katanforoosh and Daniel Kunin},
    title = {Initializing neural networks},
    url = {https://www.deeplearning.ai/ai-notes/initialization/index.html},
    year = {2019},
    note = {Accessed on March 24, 2024},
}

%%%%%%%%%%%%%%%%%  Published scientific articles  %%%%%%%%%%%%%%%%%%%


@article{Cristoforetti:2012su,
    author = "Cristoforetti, Marco and Di Renzo, Francesco and Scorzato, Luigi",
    collaboration = "AuroraScience",
    title = "{New approach to the sign problem in quantum field theories: High density QCD on a Lefschetz thimble}",
    doi = "10.1103/PhysRevD.86.074506",
    journal = "Phys. Rev. D",
    volume = "86",
    pages = "074506",
    year = "2012"
}


%%%%%%%%%%%%%%%%%  Books  %%%%%%%%%%%%%%%%%%%

@book{Goodfellow2016,
author ={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
title = {Deep Learning},
publisher = {The MIT Press, Cambridge, Massachusetts},
year = {2016}
}

@book{Bishop2006,
author = {Bishop, Christopher M.},
title = {Pattern Recognition and Machine Learning},
year = {2006},
publisher = {Springer Verlag, Berlin}
}

@book{Hastie2009,
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  publisher = {Springer Verlag, Berlin},
  title = {The Elements of Statistical Learning: Data Mining, Inference and Prediction},
  year = {2009}
}


@book{Murphy2012,
author = {Murphy, Kevin P.},
title = {Machine Learning: A Probabilistic Perspective},
year = {2012},
publisher = {The MIT Press, Cambdridge, Massachusetts}
}

@book{Schuld2018,
author = {Schuld, Maria and Petruccione, Francesco},
title = {Supervised Learning with Quantum Computers},
year = {2018},
publisher = {Springer Verlag, Berlin}
}


@book{Burkard2012, 
    author = {R.E. Burkard and M. Dell'Amico and S. Martello},
    title = {Assignment Problems},
    publisher = {SIAM, Philadelphia, USA},
    year = {2012}
}    

@BOOK{NAS-Models2012,
  author    = "{National Research Council}",
  title     = "Assessing the Reliability of Complex Models: Mathematical and Statistical Foundations of Verification, Validation, and Uncertainty Quantification",
  isbn      = "978-0-309-25634-6",
  doi       = "10.17226/13395",
  url       = "https://www.nap.edu/catalog/13395/assessing-the-reliability-of-complex-models-mathematical-and-statistical-foundations",
  year      = 2012,
  publisher = "The National Academies Press",
  address   = "Washington, DC"
}

%%%%%%%%%%%%%%%%%  REPORTS and THESES %%%%%%%%%%%%%%%%%%%

@techreport{Young2009,
  title = {Scientific grand challenges: forefront questions in nuclear science and the role of computing at the extreme scale},
  author = {Young, G. and Dean, D.J. and Savage, M.J.},
  year = {2009},
  address = {{Washington, D.C.}},
  institution = {U.S. Department of Energy},
  url="https://science.osti.gov/-/media/ascr/pdf/program-documents/docs/Np_report.pdf"
}



@phdthesis{Bradt2017,
author = {Bradt, Joshua William},
school = {Michigan State University},
title = {{Measurement of isobaric analogue resonances of 47Ar with the active target time projection chamber}},
URL={http://publications.nscl.msu.edu/thesis/\%20Brandt_2017_5279.pdf},
year = {2017}
}


%%%%%%%%%%%%%%%%%  Preprints %%%%%%%%%%%%%%%%%%%%%%%%%

@misc{pescia2021,
      title={Neural-Network Quantum States for Periodic Systems in Continuous Space}, 
      author={Gabriel Pescia and Jiequn Han and Alessandro Lovato and Jianfeng Lu and Giuseppe Carleo},
      year={2021},
      eprint={2112.11957},
      archivePrefix={arXiv},
      primaryClass={quant-ph}
}


%%%%%%%%%%%%%%%%%  Conference proceedings  %%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{Chen:2021jey,
    author = "Chen, Shi-Yang and Ding, Heng-Tong and Liu, Fei-Yi and Papp, Gabor and Yang, Chun-Bin",
    title = "{Machine learning Hadron Spectral Functions in Lattice QCD}",
    eprint = "2112.00460",
    archivePrefix = "arXiv",
    primaryClass = "hep-lat",
    month = "12",
    year = "2021"
}

